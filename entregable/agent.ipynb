{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "%run environment.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pandas\n",
    "random.seed(5)\n",
    "\n",
    "        \n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=0.9, episodes=1):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.environment = env\n",
    "        self.qtable = self.__initdic__() #rewards table\n",
    "        self.episodes = episodes\n",
    "\n",
    "\n",
    "    def __initdic__(self):\n",
    "        table = dict()\n",
    "        # Initialize Q table with 0 for each state-action pair\n",
    "        for state in self.environment.get_states():\n",
    "            table[state] = np.zeros(len(self.environment.get_possible_actions(state)))\n",
    "        return table\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        start = time.time()\n",
    "        stats = []\n",
    "        for counter in range(self.episodes):\n",
    "            start_time = time.time()\n",
    "            done = False\n",
    "            iterations = 0\n",
    "            values_updates = 0\n",
    "            \n",
    "            while not done:\n",
    "                iterations += 1\n",
    "                current_state = copy.deepcopy(self.environment.get_current_state())\n",
    "                if random.uniform(0,1) < self.epsilon:\n",
    "                    action = self.random_action(current_state)\n",
    "                else:\n",
    "                    action = self.max_action(current_state)\n",
    "                action_index = self.action_index(action)\n",
    "                next_state, reward, done = self.step(action)\n",
    "                \n",
    "                if not done:\n",
    "                    old_value = self.qtable[current_state][action_index]\n",
    "                    next_max = np.max(self.qtable[next_state])\n",
    "                    new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "\n",
    "                    # Llevo un contador de la cantidad de veces en las que los q-valores cambiaron.\n",
    "                    # Esto es útil para las gráficas de convergencia.\n",
    "                    if self.qtable[current_state][action_index] != new_value:\n",
    "                        values_updates += 1\n",
    "\n",
    "                    self.qtable[current_state][action_index] = new_value\n",
    "                \n",
    "                else:\n",
    "                    # Llevo un contador de la cantidad de veces en las que los q-valores cambiaron.\n",
    "                    # Esto es útil para las gráficas de convergencia.\n",
    "                    if self.qtable[current_state][action_index] != reward:\n",
    "                        values_updates += 1\n",
    "\n",
    "                    self.qtable[current_state][action_index] = reward\n",
    "\n",
    "            execution_time = round((time.time() - start_time) * 1000, 6)\n",
    "            stats.append([counter, iterations, values_updates, execution_time])\n",
    "            \n",
    "            if counter % 30 == 0:\n",
    "                self.epsilon -= self.epsilon/10\n",
    "            self.environment.reset()\n",
    "        stats_df = pandas.DataFrame(stats, columns=['Episodio', 'Iteraciones', 'Cantidad de cambios en los q-valores', 'Tiempo de ejecución (ms)'])\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        return stats_df, duration\n",
    "\n",
    "\n",
    "    def random_action(self, current_state):\n",
    "        return random.choice(self.environment.get_possible_actions(current_state))\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        old_state = copy.deepcopy(self.environment.get_current_state())\n",
    "        reward, new_state, done = self.environment.do_action(action)\n",
    "        next_state = copy.deepcopy(new_state)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "\n",
    "    def actions_values(self):\n",
    "        actions = {}\n",
    "        self.environment.reset()\n",
    "        values = [[0 for _ in range(self.environment.ncols)] for _ in range(self.environment.nrows)]\n",
    "        for state in self.environment.get_states():\n",
    "            action = np.argmax(self.qtable[state])\n",
    "            actions[state] = self.environment.get_possible_actions(state)[action]\n",
    "            (i, j) = state\n",
    "            values[i][j] = np.max(self.qtable[state])\n",
    "        return actions, values\n",
    "        \n",
    "\n",
    "    def max_action(self, current_state):\n",
    "        action_index = np.argmax(self.qtable[current_state]) \n",
    "        actions = self.environment.actions\n",
    "        return actions[action_index]\n",
    "\n",
    "    def action_name(self, action_index):\n",
    "        return self.environment.actions[action_index]\n",
    "    \n",
    "    \n",
    "    def action_index(self, action):\n",
    "        actions = self.environment.actions\n",
    "        for i in range(len(actions)):\n",
    "            if actions[i] == action:\n",
    "                return i\n",
    "        return -1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
